{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feef4554-c47a-46f9-ae58-f5fe26b15af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DASK client set\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from dask.distributed import Client\n",
    "# client = Client(scheduler_file='/proj/kimyy/Dropbox/source/python/all/mpi/scheduler.json', threads_per_worker=2, n_workers=6)\n",
    "client = Client(scheduler_file='/proj/kimyy/Dropbox/source/python/all/mpi/scheduler.json')\n",
    "# client = Client(scheduler_file='/proj/kimyy/Dropbox/source/python/all/mpi/scheduler_10.json')  \n",
    "\n",
    "# add private module path for workers\n",
    "# client.run(lambda: os.environ.update({'PYTHONPATH': '/proj/kimyy/Dropbox/source/python/all/Modules/CESM2'}))\n",
    "# def add_path():\n",
    "#     if '/proj/kimyy/Dropbox/source/python/all/Modules/CESM2' not in sys.path:\n",
    "#         sys.path.append('/proj/kimyy/Dropbox/source/python/all/Modules/CESM2')\n",
    "\n",
    "# client.run(add_path)\n",
    "\n",
    "def setup_module_path():\n",
    "    module_path = '/proj/kimyy/Dropbox/source/python/all/Modules/CESM2'\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "\n",
    "client.run(setup_module_path)\n",
    "\n",
    "client\n",
    "\n",
    "# get path for path changes in Jupyter notebook: File - Open from Path - insert relative_path\n",
    "notebook_path = os.path.abspath(\".\")\n",
    "_, _, relative_path = notebook_path.partition('/all/')\n",
    "relative_path = '/all/' + relative_path\n",
    "relative_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a775867-1d6a-49ee-8ab4-d1ce231535e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load public modules\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.path as mpath\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from scipy import stats\n",
    "from scipy.interpolate import griddata\n",
    "import cmocean\n",
    "from cmcrameri import cm\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "import pandas as pd\n",
    "import cftime\n",
    "import pop_tools\n",
    "from pprint import pprint\n",
    "import time\n",
    "import subprocess\n",
    "import re as re_mod\n",
    "import cftime\n",
    "import datetime\n",
    "from scipy.stats import ttest_1samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f0670f-d577-4e25-a1ee-7dafe4aab3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load private modules\n",
    "\n",
    "import sys\n",
    "sys.path.append('/proj/kimyy/Dropbox/source/python/all/Modules/CESM2')\n",
    "from KYY_CESM2_NWP_preprocessing import CESM2_NWP_config\n",
    "# import KYY_CESM2_preprocessing\n",
    "# import importlib\n",
    "# importlib.reload(KYY_CESM2_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac90ed9f-2696-4c83-b514-e794096a5e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DIC (Total)\n",
    "cfg_var_DIC=CESM2_NWP_config()\n",
    "cfg_var_DIC.year_s=1955\n",
    "cfg_var_DIC.year_e=2020\n",
    "cfg_var_DIC.setvar('DIC')\n",
    "\n",
    "# 2. WVEL (Total)\n",
    "cfg_var_WVEL=CESM2_NWP_config()\n",
    "cfg_var_WVEL.year_s=1955\n",
    "cfg_var_WVEL.year_e=2020\n",
    "cfg_var_WVEL.setvar('WVEL')\n",
    "\n",
    "if cfg_var_DIC.comp=='ocn':\n",
    "    ds_grid = pop_tools.get_grid('POP_gx1v7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c655962-37a2-4ea2-9a63-8e68084ba126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define preprocessing function\n",
    "\n",
    "exceptcv=['time','lon','lat','lev', 'TAREA', 'TLONG', 'TLAT', 'z_t', 'z_t_2', cfg_var_DIC.var, cfg_var_TEMP.var]\n",
    "# exceptcv=['time','lon','lat','lev', 'TAREA', 'TLONG', 'TLAT', 'z_t', 'dz', 'z_t_2', cfg_var_DIC.var, cfg_var_TEMP.var]\n",
    "\n",
    "def process_coords_3d(ds, sd, ed, drop=True, except_coord_vars=exceptcv):\n",
    "    \"\"\"Preprocessor function to drop all non-dim coords, which slows down concatenation.\"\"\"\n",
    "    coord_vars = []\n",
    "    for v in np.array(ds.coords) :\n",
    "        if not v in except_coord_vars:\n",
    "            coord_vars += [v]\n",
    "    for v in np.array(ds.data_vars) :\n",
    "        if not v in except_coord_vars:\n",
    "            coord_vars += [v]\n",
    "\n",
    "    if drop:\n",
    "        ds= ds.drop(coord_vars)\n",
    "        ds= ds.sel(time=slice(sd, ed))\n",
    "        # ds = ds.isel(z_t=slice(0, 39)) # ~39 layer (1000m)\n",
    "        # ds = (ds.isel(z_t=slice(1, 39)) * ds.dz).sum(dim='z_t') / ds.dz.sum(dim='z_t')\n",
    "        return ds\n",
    "    else:\n",
    "        return ds.set_coords(coord_vars)\n",
    "\n",
    "\n",
    "\n",
    "def process_coords_3d_LE(ds, sd, ed, drop=True, except_coord_vars=exceptcv):\n",
    "    \"\"\"\n",
    "    Preprocessor function for CESM POP-style datasets.\n",
    "    - Normalizes vertical coordinate: if z_t or z_t_2 exists, rename to 'depth'.\n",
    "    - Replaces its values with z_t_new for consistency.\n",
    "    - Optionally drops unnecessary coordinate variables for faster concatenation.\n",
    "    \"\"\"\n",
    "    z_t_new = np.array([5.0000000e+00, 1.5000000e+01, 2.5000000e+01, 3.5000000e+01,\n",
    "       4.5000000e+01, 5.5000000e+01, 6.5000000e+01, 7.5000000e+01,\n",
    "       8.5000000e+01, 9.5000000e+01, 1.0500000e+02, 1.1500000e+02,\n",
    "       1.2500000e+02, 1.3500000e+02, 1.4500000e+02, 1.5500000e+02,\n",
    "       1.6509839e+02, 1.7547903e+02, 1.8629126e+02, 1.9766026e+02,\n",
    "       2.0971138e+02, 2.2257828e+02, 2.3640883e+02, 2.5137015e+02,\n",
    "       2.6765421e+02, 2.8548364e+02, 3.0511920e+02, 3.2686798e+02,\n",
    "       3.5109348e+02, 3.7822760e+02, 4.0878464e+02, 4.4337769e+02,\n",
    "       4.8273669e+02, 5.2772797e+02, 5.7937286e+02, 6.3886261e+02,\n",
    "       7.0756329e+02, 7.8700250e+02, 8.7882520e+02, 9.8470581e+02,\n",
    "       1.1062042e+03, 1.2445669e+03, 1.4004972e+03, 1.5739464e+03,\n",
    "       1.7640033e+03, 1.9689442e+03, 2.1864565e+03, 2.4139714e+03,\n",
    "       2.6490012e+03, 2.8893845e+03, 3.1334045e+03, 3.3797935e+03,\n",
    "       3.6276702e+03, 3.8764519e+03, 4.1257681e+03, 4.3753926e+03,\n",
    "       4.6251904e+03, 4.8750835e+03, 5.1250278e+03, 5.3750000e+03])\n",
    "    \n",
    "    # ------------------------------------------------------\n",
    "    # 1️⃣ Normalize vertical coordinate name\n",
    "    # ------------------------------------------------------\n",
    "    if \"z_t_2\" in ds.dims:\n",
    "        ds = ds.rename({\"z_t_2\": \"depth\"})\n",
    "    elif \"z_t\" in ds.dims:\n",
    "        ds = ds.rename({\"z_t\": \"depth\"})\n",
    "    else:\n",
    "        print(\"[Warning] No vertical coordinate (z_t or z_t_2) found — skipped.\")\n",
    "        return ds\n",
    "\n",
    "    # Drop any leftover z_t/z_t_2 coordinate variable if it exists\n",
    "    ds = ds.drop_vars([\"z_t\", \"z_t_2\"], errors=\"ignore\")\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # 2️⃣ Replace coordinate values with z_t_new\n",
    "    # ------------------------------------------------------\n",
    "    if \"depth\" in ds.coords:\n",
    "        if len(ds[\"depth\"]) == len(z_t_new):\n",
    "            ds = ds.assign_coords(depth=z_t_new)\n",
    "        else:\n",
    "            print(f\"[Warning] depth length mismatch: {len(ds['depth'])} vs {len(z_t_new)}\")\n",
    "    else:\n",
    "        print(\"[Warning] depth coordinate missing after renaming.\")\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # 3️⃣ Clean up coordinate references inside variable attributes\n",
    "    # ------------------------------------------------------\n",
    "    for v in ds.data_vars:\n",
    "        if \"coordinates\" in ds[v].attrs:\n",
    "            ds[v].attrs[\"coordinates\"] = (\n",
    "                ds[v].attrs[\"coordinates\"]\n",
    "                .replace(\"z_t_2\", \"depth\")\n",
    "                .replace(\"z_t\", \"depth\")\n",
    "            )\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # 4️⃣ Drop unnecessary coordinate variables and slice time\n",
    "    # ------------------------------------------------------\n",
    "    coord_vars = []\n",
    "    for v in np.array(ds.coords):\n",
    "        if v not in except_coord_vars:\n",
    "            coord_vars.append(v)\n",
    "    for v in np.array(ds.data_vars):\n",
    "        if v not in except_coord_vars:\n",
    "            coord_vars.append(v)\n",
    "\n",
    "    if drop:\n",
    "        ds = ds.drop(coord_vars, errors=\"ignore\")\n",
    "        ds = ds.sel(time=slice(sd, ed))\n",
    "    else:\n",
    "        ds = ds.set_coords(coord_vars)\n",
    "\n",
    "    return ds\n",
    "\n",
    "start_date = cftime.DatetimeNoLeap(cfg_var_DIC.year_s, 2, 1)\n",
    "end_date = cftime.DatetimeNoLeap(cfg_var_DIC.year_e+1, 1, 1)\n",
    "\n",
    "\n",
    "# ds = ds.isel(lev=slice(1, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d604974d-aed7-4ce2-b5b7-e8fa43bf85ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read LE dataset\n",
    "\n",
    "# Quicker test for 2 ensembles only\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#DIC\n",
    "cfg_var_DIC.LE_path_load(cfg_var_DIC.var)\n",
    "cfg_var_DIC.LE_ds = xr.open_mfdataset(cfg_var_DIC.LE_file_list[0][11:12], \n",
    "                       chunks={'time': 12}, \n",
    "                       combine='nested', \n",
    "                       concat_dim=[[*cfg_var_DIC.LE_ensembles][11:12], 'time'], \n",
    "                       parallel=True,\n",
    "                       preprocess=lambda ds: process_coords_3d_LE(ds, start_date, end_date),\n",
    "                       decode_cf=True, \n",
    "                       decode_times=True)         \n",
    "\n",
    "cfg_var_DIC.LE_ds = cfg_var_DIC.LE_ds.rename({\"concat_dim\": \"ens_LE\"})\n",
    "new_time = cfg_var_DIC.LE_ds.time - np.array([datetime.timedelta(days=15)] * len(cfg_var_DIC.LE_ds.time))\n",
    "cfg_var_DIC.LE_ds = cfg_var_DIC.LE_ds.assign_coords(time=new_time)\n",
    "cfg_var_DIC.LE_ds = cfg_var_DIC.LE_ds.rename({\"depth\": \"z_t\"})\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print('elasped time for reading LE: ' + str(elapsed_time))\n",
    "\n",
    "# Read ADA dataset\n",
    "\n",
    "# Quicker test for 2 ensembADAs only\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#DIC\n",
    "cfg_var_DIC.ADA_path_load(cfg_var_DIC.var)\n",
    "cfg_var_DIC.ADA_ds = xr.open_mfdataset(cfg_var_DIC.ADA_file_list[0][5:6], \n",
    "                       chunks={'time': 12}, \n",
    "                       combine='nested', \n",
    "                       concat_dim=[[*cfg_var_DIC.ADA_ensembles][5:6], 'time'], \n",
    "                       parallel=True,\n",
    "                       preprocess=lambda ds: process_coords_3d(ds, start_date, end_date),\n",
    "                       decode_cf=True, \n",
    "                       decode_times=True)         \n",
    "\n",
    "cfg_var_DIC.ADA_ds = cfg_var_DIC.ADA_ds.rename({\"concat_dim\": \"ens_ADA\"})\n",
    "new_time = cfg_var_DIC.ADA_ds.time - np.array([datetime.timedelta(days=15)] * len(cfg_var_DIC.ADA_ds.time))\n",
    "cfg_var_DIC.ADA_ds = cfg_var_DIC.ADA_ds.assign_coords(time=new_time)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print('elasped time for reading ADA: ' + str(elapsed_time))\n",
    "\n",
    "# Read WDA dataset\n",
    "\n",
    "# Quicker test for 2 ensembles only\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#DIC\n",
    "cfg_var_DIC.WDA_path_load(cfg_var_DIC.var)\n",
    "cfg_var_DIC.WDA_ds = xr.open_mfdataset(cfg_var_DIC.WDA_file_list[0], \n",
    "                       chunks={'time': 12}, \n",
    "                       combine='nested', \n",
    "                       concat_dim=[[*cfg_var_DIC.WDA_ensembles], 'time'], \n",
    "                       parallel=True,\n",
    "                       preprocess=lambda ds: process_coords_3d(ds, start_date, end_date),\n",
    "                       decode_cf=True, \n",
    "                       decode_times=True)         \n",
    "\n",
    "cfg_var_DIC.WDA_ds = cfg_var_DIC.WDA_ds.rename({\"concat_dim\": \"ens_WDA\"})\n",
    "new_time = cfg_var_DIC.WDA_ds.time - np.array([datetime.timedelta(days=15)] * len(cfg_var_DIC.WDA_ds.time))\n",
    "cfg_var_DIC.WDA_ds = cfg_var_DIC.WDA_ds.assign_coords(time=new_time)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print('elasped time for reading WDA: ' + str(elapsed_time))\n",
    "\n",
    "# Read ODA dataset\n",
    "\n",
    "# Quicker test for 2 ensembles only\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#DIC\n",
    "cfg_var_DIC.ODA_path_load(cfg_var_DIC.var)\n",
    "cfg_var_DIC.ODA_ds = xr.open_mfdataset(cfg_var_DIC.ODA_file_list[0][15:16], \n",
    "                       chunks={'time': 12}, \n",
    "                       combine='nested', \n",
    "                       concat_dim=[[*cfg_var_DIC.ODA_ensembles][15:16], 'time'], \n",
    "                       parallel=True,\n",
    "                       preprocess=lambda ds: process_coords_3d(ds, start_date, end_date),\n",
    "                       decode_cf=True, \n",
    "                       decode_times=True)      \n",
    "\n",
    "cfg_var_DIC.ODA_ds = cfg_var_DIC.ODA_ds.rename({\"concat_dim\": \"ens_ODA\"})\n",
    "new_time = cfg_var_DIC.ODA_ds.time - np.array([datetime.timedelta(days=15)] * len(cfg_var_DIC.ODA_ds.time))\n",
    "cfg_var_DIC.ODA_ds = cfg_var_DIC.ODA_ds.assign_coords(time=new_time)\n",
    "\n",
    "\n",
    "# cfg_var_DIC.ODA_path_load(cfg_var_DIC.var)\n",
    "# cfg_var_DIC.ODA_file_list[0]\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print('elasped time for reading ODA: ' + str(elapsed_time))\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#regrids\n",
    "import xcesm\n",
    "\n",
    "lat_range = slice(10, 60)\n",
    "lon_range = slice(110, 190)\n",
    "\n",
    "cfg_var_DIC.LE_ds_rgd = (\n",
    "    cfg_var_DIC.LE_ds['DIC']\n",
    "    .isel(ens_LE=0)\n",
    "    .utils.regrid()\n",
    "    .sel(lat=lat_range, lon=lon_range)\n",
    ")\n",
    "\n",
    "cfg_var_DIC.ODA_ds_rgd = (\n",
    "    cfg_var_DIC.ODA_ds['DIC']\n",
    "    .isel(ens_ODA=0)\n",
    "    .utils.regrid()\n",
    "    .sel(lat=lat_range, lon=lon_range)\n",
    ")\n",
    "\n",
    "cfg_var_DIC.ADA_ds_rgd = (\n",
    "    cfg_var_DIC.ADA_ds['DIC']\n",
    "    .isel(ens_ADA=0)\n",
    "    .utils.regrid()\n",
    "    .sel(lat=lat_range, lon=lon_range)\n",
    ")\n",
    "\n",
    "cfg_var_DIC.WDA_ds_rgd = (\n",
    "    cfg_var_DIC.WDA_ds['DIC']\n",
    "    .isel(ens_WDA=0)\n",
    "    .utils.regrid()\n",
    "    .sel(lat=lat_range, lon=lon_range)\n",
    ")\n",
    "\n",
    "cfg_var_DIC.LE_ds_rgd = cfg_var_DIC.LE_ds_rgd.sortby(\"time\")\n",
    "cfg_var_DIC.WDA_ds_rgd = cfg_var_DIC.WDA_ds_rgd.sortby(\"time\")\n",
    "cfg_var_DIC.ADA_ds_rgd = cfg_var_DIC.ADA_ds_rgd.sortby(\"time\")\n",
    "cfg_var_DIC.ODA_ds_rgd = cfg_var_DIC.ODA_ds_rgd.sortby(\"time\")\n",
    "\n",
    "\n",
    "cfg_var_DIC.LE_ds_rgd = cfg_var_DIC.LE_ds_rgd.assign_coords(z_t = cfg_var_DIC.ODA_ds_rgd.z_t)\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print('elasped time for regriding: ' + str(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62c3035-eb9d-4fe2-aabb-77246da6ee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read LE dataset\n",
    "\n",
    "# Quicker test for 2 ensembles only\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#WVEL\n",
    "cfg_var_WVEL.LE_path_load(cfg_var_WVEL.var)\n",
    "cfg_var_WVEL.LE_ds = xr.open_mfdataset(cfg_var_WVEL.LE_file_list[0][11:12], \n",
    "                       chunks={'time': 12}, \n",
    "                       combine='nested', \n",
    "                       concat_dim=[[*cfg_var_WVEL.LE_ensembles][11:12], 'time'], \n",
    "                       parallel=True,\n",
    "                       preprocess=lambda ds: process_coords_3d_LE(ds, start_date, end_date),\n",
    "                       decode_cf=True, \n",
    "                       decode_times=True)         \n",
    "\n",
    "cfg_var_WVEL.LE_ds = cfg_var_WVEL.LE_ds.rename({\"concat_dim\": \"ens_LE\"})\n",
    "new_time = cfg_var_WVEL.LE_ds.time - np.array([datetime.timedelta(days=15)] * len(cfg_var_WVEL.LE_ds.time))\n",
    "cfg_var_WVEL.LE_ds = cfg_var_WVEL.LE_ds.assign_coords(time=new_time)\n",
    "cfg_var_WVEL.LE_ds = cfg_var_WVEL.LE_ds.rename({\"depth\": \"z_t\"})\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print('elasped time for reading LE: ' + str(elapsed_time))\n",
    "\n",
    "# Read ADA dataset\n",
    "\n",
    "# Quicker test for 2 ensembADAs only\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#WVEL\n",
    "cfg_var_WVEL.ADA_path_load(cfg_var_WVEL.var)\n",
    "cfg_var_WVEL.ADA_ds = xr.open_mfdataset(cfg_var_WVEL.ADA_file_list[0][5:6], \n",
    "                       chunks={'time': 12}, \n",
    "                       combine='nested', \n",
    "                       concat_dim=[[*cfg_var_WVEL.ADA_ensembles][5:6], 'time'], \n",
    "                       parallel=True,\n",
    "                       preprocess=lambda ds: process_coords_3d(ds, start_date, end_date),\n",
    "                       decode_cf=True, \n",
    "                       decode_times=True)         \n",
    "\n",
    "cfg_var_WVEL.ADA_ds = cfg_var_WVEL.ADA_ds.rename({\"concat_dim\": \"ens_ADA\"})\n",
    "new_time = cfg_var_WVEL.ADA_ds.time - np.array([datetime.timedelta(days=15)] * len(cfg_var_WVEL.ADA_ds.time))\n",
    "cfg_var_WVEL.ADA_ds = cfg_var_WVEL.ADA_ds.assign_coords(time=new_time)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print('elasped time for reading ADA: ' + str(elapsed_time))\n",
    "\n",
    "# Read WDA dataset\n",
    "\n",
    "# Quicker test for 2 ensembles only\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#WVEL\n",
    "cfg_var_WVEL.WDA_path_load(cfg_var_WVEL.var)\n",
    "cfg_var_WVEL.WDA_ds = xr.open_mfdataset(cfg_var_WVEL.WDA_file_list[0], \n",
    "                       chunks={'time': 12}, \n",
    "                       combine='nested', \n",
    "                       concat_dim=[[*cfg_var_WVEL.WDA_ensembles], 'time'], \n",
    "                       parallel=True,\n",
    "                       preprocess=lambda ds: process_coords_3d(ds, start_date, end_date),\n",
    "                       decode_cf=True, \n",
    "                       decode_times=True)         \n",
    "\n",
    "cfg_var_WVEL.WDA_ds = cfg_var_WVEL.WDA_ds.rename({\"concat_dim\": \"ens_WDA\"})\n",
    "new_time = cfg_var_WVEL.WDA_ds.time - np.array([datetime.timedelta(days=15)] * len(cfg_var_WVEL.WDA_ds.time))\n",
    "cfg_var_WVEL.WDA_ds = cfg_var_WVEL.WDA_ds.assign_coords(time=new_time)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print('elasped time for reading WDA: ' + str(elapsed_time))\n",
    "\n",
    "# Read ODA dataset\n",
    "\n",
    "# Quicker test for 2 ensembles only\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#WVEL\n",
    "cfg_var_WVEL.ODA_path_load(cfg_var_WVEL.var)\n",
    "cfg_var_WVEL.ODA_ds = xr.open_mfdataset(cfg_var_WVEL.ODA_file_list[0][15:16], \n",
    "                       chunks={'time': 12}, \n",
    "                       combine='nested', \n",
    "                       concat_dim=[[*cfg_var_WVEL.ODA_ensembles][15:16], 'time'], \n",
    "                       parallel=True,\n",
    "                       preprocess=lambda ds: process_coords_3d(ds, start_date, end_date),\n",
    "                       decode_cf=True, \n",
    "                       decode_times=True)      \n",
    "\n",
    "cfg_var_WVEL.ODA_ds = cfg_var_WVEL.ODA_ds.rename({\"concat_dim\": \"ens_ODA\"})\n",
    "new_time = cfg_var_WVEL.ODA_ds.time - np.array([datetime.timedelta(days=15)] * len(cfg_var_WVEL.ODA_ds.time))\n",
    "cfg_var_WVEL.ODA_ds = cfg_var_WVEL.ODA_ds.assign_coords(time=new_time)\n",
    "\n",
    "\n",
    "# cfg_var_WVEL.ODA_path_load(cfg_var_WVEL.var)\n",
    "# cfg_var_WVEL.ODA_file_list[0]\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print('elasped time for reading ODA: ' + str(elapsed_time))\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#regrids\n",
    "import xcesm\n",
    "\n",
    "lat_range = slice(10, 60)\n",
    "lon_range = slice(110, 190)\n",
    "\n",
    "cfg_var_WVEL.LE_ds_rgd = (\n",
    "    cfg_var_WVEL.LE_ds['WVEL']\n",
    "    .isel(ens_LE=0)\n",
    "    .utils.regrid()\n",
    "    .sel(lat=lat_range, lon=lon_range)\n",
    ")\n",
    "\n",
    "cfg_var_WVEL.ODA_ds_rgd = (\n",
    "    cfg_var_WVEL.ODA_ds['WVEL']\n",
    "    .isel(ens_ODA=0)\n",
    "    .utils.regrid()\n",
    "    .sel(lat=lat_range, lon=lon_range)\n",
    ")\n",
    "\n",
    "cfg_var_WVEL.ADA_ds_rgd = (\n",
    "    cfg_var_WVEL.ADA_ds['WVEL']\n",
    "    .isel(ens_ADA=0)\n",
    "    .utils.regrid()\n",
    "    .sel(lat=lat_range, lon=lon_range)\n",
    ")\n",
    "\n",
    "cfg_var_WVEL.WDA_ds_rgd = (\n",
    "    cfg_var_WVEL.WDA_ds['WVEL']\n",
    "    .isel(ens_WDA=0)\n",
    "    .utils.regrid()\n",
    "    .sel(lat=lat_range, lon=lon_range)\n",
    ")\n",
    "\n",
    "cfg_var_WVEL.LE_ds_rgd = cfg_var_WVEL.LE_ds_rgd.sortby(\"time\")\n",
    "cfg_var_WVEL.WDA_ds_rgd = cfg_var_WVEL.WDA_ds_rgd.sortby(\"time\")\n",
    "cfg_var_WVEL.ADA_ds_rgd = cfg_var_WVEL.ADA_ds_rgd.sortby(\"time\")\n",
    "cfg_var_WVEL.ODA_ds_rgd = cfg_var_WVEL.ODA_ds_rgd.sortby(\"time\")\n",
    "\n",
    "\n",
    "cfg_var_WVEL.LE_ds_rgd = cfg_var_WVEL.LE_ds_rgd.assign_coords(z_t = cfg_var_WVEL.ODA_ds_rgd.z_t)\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print('elasped time for regriding: ' + str(elapsed_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
